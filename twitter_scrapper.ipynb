{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Twitter Scraper for Data Job and AI Sentiment Analysis"
      ],
      "metadata": {
        "id": "5t_ekb4zL3qq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "This notebook is part of a sentiment analysis project aimed at understanding the public sentiment towards jobs related to data and AI. The project aims to explore people's opinions and attitudes towards data jobs, specifically in the areas of data science, data analysis, data engineering, and AI.\n",
        "\n",
        "The purpose of this notebook is to scrape tweets related to the project's scope and create a dataset that will be used for the sentiment analysis task. The notebook contains code for scraping tweets related to specific search queries, processing the scraped data, and saving it to a CSV file."
      ],
      "metadata": {
        "id": "pdGKHDgbNAaM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset description:\n",
        "\n",
        "The dataset we are trying to create will consist of tweets related to data jobs and AI. Specifically, we will be scraping tweets that contain the search terms \"data science\", \"data analysis\", \"data engineering\", \"AI\", \"artificial intelligence\", \"Chat-GPT\", \"GPT-3\", and \"GPT-4\". The dataset will be in the form of a CSV file, with columns for the tweet ID, date, content, and search query. This dataset will then be used for sentiment analysis to gain insights into public sentiment towards data jobs and AI."
      ],
      "metadata": {
        "id": "8bHeHe_uNUWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "The setup for the notebook includes importing necessary libraries and defining any necessary functions. \n",
        "\n",
        "In this notebook, we will be using the `snscrape` module to scrape tweets from Twitter, pandas to store the scraped data in a DataFrame, datetime to parse dates, tqdm to display a progress bar while scraping, and time to introduce delays while scraping to avoid being rate-limited by Twitter.\n",
        "\n",
        "We have also defined a main function called `scrape_tweets` which takes in a search term, number of tweets, language, year, and whether to save the data as a CSV file or not. \n",
        "\n",
        "This function scrapes tweets using `snscrape` and returns a DataFrame of the scraped tweets. It also saves the DataFrame as a CSV file if requested."
      ],
      "metadata": {
        "id": "ddPcmaWqNfzW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4WvhPdqmtdb"
      },
      "outputs": [],
      "source": [
        "# Installing required module\n",
        "%%capture\n",
        "!pip install pandas\n",
        "!pip install snscrape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scraping Tweets\n",
        "\n",
        "The below code scrapes Twitter for tweets containing various search terms related to data science, data analysis, data engineering, and artificial intelligence using the snscrape library. \n",
        "\n",
        "The `scrape_tweets` function takes in the search term, the number of tweets to scrape, the language of the tweets, the year to search for tweets (if provided), and whether to save the scraped data as a CSV file.\n",
        "\n",
        "For each search term, the function is called with a specified number of tweets to scrape, year, and save_file flag. \n",
        "\n",
        "The scraped data is saved in a CSV file with the filename as the search term and year (if provided).\n",
        "\n",
        "After scraping, the code reads in all the CSV files in a specified directory, adds a `query_term` and `job_title` column to each DataFrame, and merges all the DataFrames into one final DataFrame. \n",
        "\n",
        "The final DataFrame contains all the scraped tweets along with their respective search term and job title."
      ],
      "metadata": {
        "id": "m7_UB5UTPFDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import snscrape.modules.twitter as sntwitter\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import os\n",
        "\n",
        "def scrape_tweets(search_term: str, \n",
        "                  number_of_tweets: int = 1000, \n",
        "                  language: str = 'en', \n",
        "                  year: int = None, \n",
        "                  save_file: bool = False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Scrape tweets using snscrape module and return a dataframe of the scraped tweets.\n",
        "    \n",
        "    Args:\n",
        "        search_term (str): The search term to query on Twitter.\n",
        "        number_of_tweets (int): The number of tweets to scrape. Defaults to 1000.\n",
        "        language (str): The language of the tweets to scrape. Defaults to 'en'.\n",
        "        year (int): The year to search for tweets. If provided, the query will search for tweets from the given year till the current date. Defaults to None.\n",
        "        save_file (bool): Whether to save the dataframe as a CSV file. Defaults to False.\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: A dataframe containing the scraped tweets with columns ['id', 'date', 'content'].\n",
        "    \"\"\"\n",
        "    \n",
        "    # Set the query string based on the search term, language, and year\n",
        "    query = f\"{search_term} lang:{language}\"\n",
        "    if year is not None:\n",
        "        try:\n",
        "            since_date = datetime(year=year, month=1, day=1).strftime('%Y-%m-%d')\n",
        "            query += f\" since:{since_date}\"\n",
        "        except ValueError:\n",
        "            print(f\"Invalid year provided: {year}\")\n",
        "            return pd.DataFrame()\n",
        "    \n",
        "    # Use snscrape to scrape tweets\n",
        "    tweets = []\n",
        "    for i, tweet in tqdm(enumerate(sntwitter.TwitterSearchScraper(query).get_items()), total=number_of_tweets):\n",
        "\n",
        "        if i >= number_of_tweets:\n",
        "            break\n",
        "        tweets.append([tweet.id, tweet.date, tweet.rawContent])\n",
        "        \n",
        "    # Convert list of tweets to a pandas dataframe\n",
        "    df = pd.DataFrame(tweets, columns=['id', 'date', 'content'])\n",
        "    \n",
        "    # Save the dataframe as a CSV file if requested\n",
        "    if save_file:\n",
        "        filename = f\"{search_term}_{year}\" if year is not None else f\"{search_term}\"\n",
        "        df.to_csv(f\"{filename}.csv\", index=False)\n",
        "    \n",
        "    return df\n"
      ],
      "metadata": {
        "id": "PSXk6j2fm9bF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the list of queries\n",
        "queries = [\n",
        "    \"chatgpt datascience\",\n",
        "    \"chatgpt dataanalysis\", \n",
        "    \"chatgpt dataengineering\",\n",
        "\n",
        "    \"GPT-3 datascience\", \n",
        "    \"GPT-3 dataanalysis\", \n",
        "    \"GPT-3 dataengineering\", \n",
        "\n",
        "    \"GPT-4 datascience\", \n",
        "    \"GPT-4 dataanalysis\", \n",
        "    \"GPT-4 dataengineering\", \n",
        "\n",
        "    \"openai datascience\", \n",
        "    \"openai dataanalysis\", \n",
        "    \"openai dataengineering\", \n",
        "\n",
        "    \"AI datascience\",\n",
        "    \"AI dataanalysis\", \n",
        "    \"AI dataengineering\", \n",
        "\n",
        "    \"artificialintelligence datascience\",\n",
        "    \"artificialintelligence dataanalysis\",\n",
        "    \"artificialintelligence dataengineering\", \n",
        "\n",
        "]\n",
        "\n",
        "# Loop over the queries and scrape tweets for each query\n",
        "for query in queries:\n",
        "    print(f\"Scraping tweets for query: {query}\")\n",
        "\n",
        "    # Call the scrape_tweets function to scrape tweets for the query\n",
        "    scrape_tweets(query, 10_000, year=2020, save_file=True)\n",
        "\n",
        "    # Print separator for readability\n",
        "    print(\"=\"*80)"
      ],
      "metadata": {
        "id": "CBKD2YoEnHFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16f74bc9-9cd1-4db9-98a5-b14ad24211a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping tweets for query: chatgpt datascience\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|███████▍  | 7465/10000 [05:52<01:59, 21.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Scraping tweets for query: chatgpt dataanalysis\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 160/10000 [00:05<05:40, 28.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Scraping tweets for query: chatgpt dataengineering\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 110/10000 [00:05<08:46, 18.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Scraping tweets for query: GPT-3 datascience\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 19%|█▉        | 1927/10000 [01:24<05:53, 22.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Scraping tweets for query: GPT-3 dataanalysis\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 3/10000 [00:00<48:01,  3.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Scraping tweets for query: GPT-3 dataengineering\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 19/10000 [00:02<17:43,  9.39it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Scraping tweets for query: GPT-4 datascience\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 505/10000 [00:23<07:19, 21.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Scraping tweets for query: GPT-4 dataanalysis\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 17/10000 [00:01<13:42, 12.14it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Scraping tweets for query: GPT-4 dataengineering\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 4/10000 [00:01<1:20:08,  2.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Scraping tweets for query: openai datascience\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 44%|████▎     | 4365/10000 [03:12<04:08, 22.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Scraping tweets for query: openai dataanalysis\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 28/10000 [00:01<09:22, 17.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Scraping tweets for query: openai dataengineering\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 28/10000 [00:02<15:11, 10.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Scraping tweets for query: AI datascience\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [08:34<00:00, 19.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Scraping tweets for query: AI dataanalysis\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 83%|████████▎ | 8296/10000 [04:38<00:57, 29.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Scraping tweets for query: AI dataengineering\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 53%|█████▎    | 5308/10000 [03:53<03:26, 22.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Scraping tweets for query: artificialintelligence datascience\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [08:18<00:00, 20.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Scraping tweets for query: artificialintelligence dataanalysis\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 45%|████▌     | 4518/10000 [03:37<04:24, 20.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Scraping tweets for query: artificialintelligence dataengineering\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 37%|███▋      | 3749/10000 [02:44<04:33, 22.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directory where the CSV files are located\n",
        "directory = '/content/'\n",
        "\n",
        "# Define a function to add query and job title columns to a dataframe\n",
        "def add_query_and_jobtitle(df, searchterm, jobtitle):\n",
        "    df['query_term'] = searchterm\n",
        "    df['job_title'] = jobtitle\n",
        "    return df\n",
        "\n",
        "# Define a dictionary to store the merged dataframes for each job title\n",
        "merged_dfs = {}\n",
        "\n",
        "# Loop over each CSV file in the directory\n",
        "for filename in os.listdir(directory):\n",
        "    \n",
        "    # Check if the file is a CSV file\n",
        "    if not filename.endswith('.csv'):\n",
        "        continue\n",
        "    \n",
        "    # Extract the search term, job title, and year from the filename\n",
        "    parts = filename[:-9].split(' ')\n",
        "    searchterm = parts[0]\n",
        "    jobtitle = parts[1]\n",
        "    \n",
        "    # Read the CSV file into a dataframe\n",
        "    filepath = os.path.join(directory, filename)\n",
        "    df = pd.read_csv(filepath)\n",
        "    \n",
        "    # Add query and job title columns to the dataframe\n",
        "    df = add_query_and_jobtitle(df, searchterm, jobtitle)\n",
        "    \n",
        "    # Merge the dataframe with the existing merged dataframe for this job title\n",
        "    if jobtitle in merged_dfs:\n",
        "        merged_dfs[jobtitle] = pd.concat([merged_dfs[jobtitle], df], ignore_index=True)\n",
        "    else:\n",
        "        merged_dfs[jobtitle] = df\n",
        "\n",
        "# Concatenate all the merged dataframes into one final dataframe\n",
        "final_df = pd.concat(merged_dfs.values(), ignore_index=True)\n",
        "\n",
        "# Print the final dataframe\n",
        "print('Final merged dataframe:')\n",
        "final_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "NpsdtRkeBznz",
        "outputId": "6d0f7bb0-17fc-4772-d3ad-d62191e92c89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final merged dataframe:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    id                       date  \\\n",
              "0  1641069233017851904  2023-03-29 13:26:09+00:00   \n",
              "1  1636501901494607873  2023-03-16 22:57:12+00:00   \n",
              "2  1623374696119996455  2023-02-08 17:34:23+00:00   \n",
              "3  1439864014528471041  2021-09-20 08:08:27+00:00   \n",
              "4  1641069233017851904  2023-03-29 13:26:09+00:00   \n",
              "\n",
              "                                             content query_term  \\\n",
              "0  RT Using GPT-3.5-Turbo and GPT-4 to Apply Text...      GPT-4   \n",
              "1  \"Ingestion solved? 🤔  #DataEngineering #AI #Op...      GPT-4   \n",
              "2  OMG! Have you heard about GPT-4? This new AI t...      GPT-4   \n",
              "3  GPT-3 and GPT-4 Could Ruin the Future Internet...      GPT-4   \n",
              "4  RT Using GPT-3.5-Turbo and GPT-4 to Apply Text...      GPT-3   \n",
              "\n",
              "         job_title  \n",
              "0  dataengineering  \n",
              "1  dataengineering  \n",
              "2  dataengineering  \n",
              "3  dataengineering  \n",
              "4  dataengineering  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d077dcb8-4363-4a60-8cee-8ae08fb61e82\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>content</th>\n",
              "      <th>query_term</th>\n",
              "      <th>job_title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1641069233017851904</td>\n",
              "      <td>2023-03-29 13:26:09+00:00</td>\n",
              "      <td>RT Using GPT-3.5-Turbo and GPT-4 to Apply Text...</td>\n",
              "      <td>GPT-4</td>\n",
              "      <td>dataengineering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1636501901494607873</td>\n",
              "      <td>2023-03-16 22:57:12+00:00</td>\n",
              "      <td>\"Ingestion solved? 🤔  #DataEngineering #AI #Op...</td>\n",
              "      <td>GPT-4</td>\n",
              "      <td>dataengineering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1623374696119996455</td>\n",
              "      <td>2023-02-08 17:34:23+00:00</td>\n",
              "      <td>OMG! Have you heard about GPT-4? This new AI t...</td>\n",
              "      <td>GPT-4</td>\n",
              "      <td>dataengineering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1439864014528471041</td>\n",
              "      <td>2021-09-20 08:08:27+00:00</td>\n",
              "      <td>GPT-3 and GPT-4 Could Ruin the Future Internet...</td>\n",
              "      <td>GPT-4</td>\n",
              "      <td>dataengineering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1641069233017851904</td>\n",
              "      <td>2023-03-29 13:26:09+00:00</td>\n",
              "      <td>RT Using GPT-3.5-Turbo and GPT-4 to Apply Text...</td>\n",
              "      <td>GPT-3</td>\n",
              "      <td>dataengineering</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d077dcb8-4363-4a60-8cee-8ae08fb61e82')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d077dcb8-4363-4a60-8cee-8ae08fb61e82 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d077dcb8-4363-4a60-8cee-8ae08fb61e82');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the final dataframe\n",
        "final_df.to_csv(\"Twitter_Final_data.csv\", index=False)"
      ],
      "metadata": {
        "id": "IEKLBRskJ0lf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}